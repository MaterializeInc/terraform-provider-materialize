---
# generated by https://github.com/hashicorp/terraform-plugin-docs
# template file: templates/guides/materialize_source_table.md.tmpl
page_title: "Source Table Migration Guide"
subcategory: ""
description: |-
  Guide for migrating to the new materialize_source_table_{source_type} resources.
---

# Source versioning: migrating to `materialize_source_table_{source_type}` Resource

In previous versions of the Materialize Terraform provider, source tables were defined within the source resource itself and were considered subsources of the source rather than separate entities.

This guide will walk you through the process of migrating your existing source table definitions to the new `materialize_source_table_{source_type}` resource.

For each MySQL, Postgres, and SQL Server source, you will need to create a new `materialize_source_table_{source_type}` resource for each table that was previously defined within the source resource. This ensures that the tables are preserved during the migration process. For Kafka sources, you will need to create a `materialize_source_table_kafka` table with the same name as the kafka source to contain the data for the kafka topic.

## Old Approach

Previously, source tables were defined directly within the source resource:

### Example: MySQL Source

```hcl
resource "materialize_source_mysql" "mysql_source" {
  name         = "mysql_source"
  cluster_name = "cluster_name"

  mysql_connection {
    name = materialize_connection_mysql.mysql_connection.name
  }

  table {
    upstream_name        = "mysql_table1"
    upstream_schema_name = "shop"
    name                 = "mysql_table1_local"
  }
}
```

### Example: SQL Server Source

```hcl
resource "materialize_source_sqlserver" "sqlserver_source" {
  name         = "sqlserver_source"
  cluster_name = "cluster_name"

  sqlserver_connection {
    name = materialize_connection_sqlserver.sqlserver_connection.name
  }

  table {
    upstream_name        = "table1"
    upstream_schema_name = "dbo"
    name                 = "sqlserver_table1"
  }

  exclude_columns = ["dbo.table1.about"]
}
```

### Example: Kafka Source

```hcl
resource "materialize_source_kafka" "example_source_kafka_format_text" {
  name         = "source_kafka_text"
  comment      = "source kafka comment"
  cluster_name = materialize_cluster.cluster_source.name
  topic        = "topic1"

  kafka_connection {
    name          = materialize_connection_kafka.kafka_connection.name
    schema_name   = materialize_connection_kafka.kafka_connection.schema_name
    database_name = materialize_connection_kafka.kafka_connection.database_name
  }
  key_format {
    text = true
  }
  value_format {
    text = true
  }
}
```

## New Approach

The new approach separates source definitions and table definitions. You will now create the source without specifying the tables, and then define each table using the `materialize_source_table_{source_type}` resource.

## Manual Migration Process

This manual migration process requires users to create new source tables using the new `materialize_source_table_{source_type}` resource and then remove the old ones. We'll cover examples for MySQL, SQL Server, and Kafka sources.

### Step 1: Define `materialize_source_table_{source_type}` Resources

Before making any changes to your existing source resources, create new `materialize_source_table_{source_type}` resources for each table that is currently defined within your sources.

#### MySQL Example:

```hcl
resource "materialize_source_table_mysql" "mysql_table_from_source" {
  name           = "mysql_table1_from_source"
  schema_name    = "public"
  database_name  = "materialize"

  source {
    name = materialize_source_mysql.mysql_source.name
    // Define the schema and database for the source if needed
  }

  upstream_name        = "mysql_table1"
  upstream_schema_name = "shop"

  ignore_columns = ["about"]
}
```

#### SQL Server Example:

```hcl
resource "materialize_source_table_sqlserver" "sqlserver_table_from_source" {
  name           = "sqlserver_table1_from_source"
  schema_name    = "public"
  database_name  = "materialize"

  source {
    name = materialize_source_sqlserver.sqlserver_source.name
    // Define the schema and database for the source if needed
  }

  upstream_name        = "table1"
  upstream_schema_name = "dbo"

  text_columns    = ["dbo.table1.about"]
  exclude_columns = ["dbo.table1.id"]
}
```

#### Kafka Example:

```hcl
resource "materialize_source_table_kafka" "kafka_table_from_source" {
  name           = "kafka_table_from_source"
  schema_name    = "public"
  database_name  = "materialize"

  source_name {
    name = materialize_source_kafka.kafka_source.name
  }

  key_format {
    text = true
  }

  value_format {
    text = true
  }

}
```

### Step 2: Apply the Changes

Run `terraform plan` and `terraform apply` to create the new `materialize_source_table_{source_type}` resources.

### Step 3: Remove Table Blocks from Source Resources

Once the new `materialize_source_table_{source_type}` resources are successfully created, remove all the deprecated and table-specific attributes from your source resources.

#### MySQL Example:

For MySQL sources, remove the `table` block and any table-specific attributes from the source resource:

```hcl
resource "materialize_source_mysql" "mysql_source" {
  name         = "mysql_source"
  cluster_name = "cluster_name"

  mysql_connection {
    name = materialize_connection_mysql.mysql_connection.name
  }

  // Remove the table blocks from here
  - table {
  -   upstream_name        = "mysql_table1"
  -   upstream_schema_name = "shop"
  -   name                 = "mysql_table1_local"
  -
  -   ignore_columns = ["about"]
  -
  ...
}
```

#### SQL Server Example:

For SQL Server sources, remove the `table` block and any table-specific attributes from the source resource:

```hcl
resource "materialize_source_sqlserver" "sqlserver_source" {
  name         = "sqlserver_source"
  cluster_name = "cluster_name"

  sqlserver_connection {
    name = materialize_connection_sqlserver.sqlserver_connection.name
  }

  // Remove the table blocks from here
  - table {
  -   upstream_name        = "table1"
  -   upstream_schema_name = "dbo"
  -   name                 = "sqlserver_table1"
  - }
  -
  - exclude_columns = ["dbo.table1.about"]
}
```

#### Kafka Example:

For Kafka sources, remove the `format`, `include_key`, `include_headers`, and other table-specific attributes from the source resource:

```hcl
resource "materialize_source_kafka" "kafka_source" {
  name         = "kafka_source"
  cluster_name = "cluster_name"

  kafka_connection {
    name = materialize_connection_kafka.kafka_connection.name
  }

  topic = "example_topic"

  lifecycle {
    ignore_changes = [
      include_key,
      include_headers,
      format,
      ...
    ]
  }
  // Remove the format, include_key, include_headers, and other table-specific attributes
}
```

In the `lifecycle` block, add the `ignore_changes` meta-argument to prevent Terraform from trying to update these attributes during subsequent applies, that way Terraform won't try to update these values based on incomplete information from the state as they will no longer be defined in the source resource itself but in the new `materialize_source_table_{source_type}` resources.

### Step 4: Update Terraform State

After removing the `table` blocks and the table/topic specific attributes from your source resources, run `terraform plan` and `terraform apply` again to update the Terraform state and apply the changes.

### Step 5: Verify the Migration

After applying the changes, verify that your tables are still correctly set up in Materialize by checking the table definitions using Materialize's SQL commands.

For a more detailed view of a specific table, you can use the `SHOW CREATE TABLE` command:

```sql
SHOW CREATE TABLE materialize.public.mysql_table1_from_source;
```

## Importing Existing Tables

To import existing tables into your Terraform state, use the following command:

```bash
terraform import materialize_source_table_{source_type}.table_name <region>:<table_id>
```

Replace `{source}` with the appropriate source type (e.g., `mysql`, `kafka`), `<region>` with the actual region, and `<table_id>` with the table ID.

### Important Note on Importing

Due to limitations in the current read function, not all properties of the source tables are available when importing. To work around this, you'll need to use the `ignore_changes` lifecycle meta-argument for certain attributes that can't be read back from the state.

For example, for a Kafka source table:

```hcl
resource "materialize_source_table_kafka" "kafka_table_from_source" {
  name           = "kafka_table_from_source"
  schema_name    = "public"
  database_name  = "materialize"

  source_name = materialize_source_kafka.kafka_source.name

  include_key     = true
  include_headers = true

  envelope {
    upsert = true
  }

  lifecycle {
    ignore_changes = [
      include_key,
      include_headers,
      envelope
      ... Add other attributes here as needed
    ]
  }
}
```

This `ignore_changes` block tells Terraform to ignore changes to these attributes during subsequent applies, preventing Terraform from trying to update these values based on incomplete information from the state.

After importing, you may need to manually update these ignored attributes in your Terraform configuration to match the actual state in Materialize.

## Webhook Sources

Webhook sources have a different migration path. In the new model, webhooks are defined as tables using `CREATE TABLE ... FROM WEBHOOK` instead of `CREATE SOURCE ... FROM WEBHOOK`. This change reflects the fact that webhooks don't behave like traditional sources - they write data directly to persist without running dataflows.

In Terraform, use `materialize_table_from_webhook` instead of the legacy `materialize_source_webhook`.

| Aspect | Legacy (`materialize_source_webhook`) | New (`materialize_table_from_webhook`) |
|--------|--------------------------------------|----------------------------------------|
| SQL Object | `CREATE SOURCE ... FROM WEBHOOK` | `CREATE TABLE ... FROM WEBHOOK` |
| System Catalog | `mz_sources` | `mz_tables` |
| Cluster | Required | Not supported |

### Migration Options

There is no automated migration on the database side yet. You have two options:

#### Option 1: Wait for Automated Migration (Recommended)

The recommended approach is to continue using `materialize_source_webhook` until an automated migration path becomes available on the database side. The legacy resource remains fully supported, and this option avoids downtime and data loss.

Once automated migration is available, you'll be able to migrate over to the new `materialize_table_from_webhook` resource.

#### Option 2: Recreate Using the New Resource

If you can accept downtime and data loss, you can recreate the webhook using the new `materialize_table_from_webhook` resource:

```hcl
# New approach (no cluster required)
resource "materialize_table_from_webhook" "example_webhook_table" {
  name          = "example_webhook_table"
  schema_name   = "public"
  database_name = "materialize"
  body_format   = "json"

  include_header {
    header = "x-event-type"
    alias  = "event_type"
  }

  check_options {
    field {
      headers = true
    }
    field {
      secret {
        name = materialize_secret.webhook_secret.name
      }
      alias = "secret"
    }
  }
  check_expression = "headers->'x-mz-api-key' = secret"
}
```

**Warning:** This approach will result in:
- Loss of all existing webhook data
- Downtime while the webhook is recreated
- Need to update webhook endpoint URLs

Steps:
1. Remove the existing `materialize_source_webhook` resource and run `terraform apply` to delete it
2. Add the new `materialize_table_from_webhook` resource and run `terraform apply` to create it
3. Update any downstream dependencies (views, materialized views) to reference the new table
4. Update your webhook endpoint URL to point to the new table
